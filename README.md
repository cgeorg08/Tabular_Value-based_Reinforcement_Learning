# Tabular_Value-based_Reinforcement_Learning

Tabular, value-based algorithms are said to be the fundamental principles of the reinforcement learning field. They are able to solve moderate-size problems, which can fit in memory, by constructing tables. We conduct experiments with algorithms based on Dynamic Programming such as Value Iteration and model-free algorithms like Q-learning, SARSA, N-step and Monte Carlo. We are interested in investigating the following topics: (a) the differences between Value Iteration and model-free algorithms, (b) the impact of various action selection policies, (c) on-policy versus off-policy algorithms and (d) the depth we perform updates. In addition, we go deeper into the hyperparameters of the Q-learning algorithm so as to observe how we can make the algorithm converge quicker to optimal policy.  

To investigate the aforementioned topics successfully, we place our agent inside the Stochastic Windy Gridworld, a modified version of the example 6.5 in page 156 of the book "Reinforcement learning : an introduction". The environment is a grid 10x7, the starting state of the agent is (0,3) and the goal state is (7,3). The agent can move towards any direction and it gets -1 as a reward, except for the case it reaches the goal state where it gets reward equal to 40. What makes the environment stochastic is the vertical wind level 1 in columns 3,4,5,8 and level 2 in columns 6,7. The level of the winds indicate the number of additional steps the agent is pushed up when reaching one of the cells in the corresponding columns. The wind blows 80\% of the times.